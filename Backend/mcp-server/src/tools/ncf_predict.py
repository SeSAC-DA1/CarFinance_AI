"""
CarFin MCP Tool: NCF Predict
Neural Collaborative Filtering ê¸°ë°˜ ì‹¤ì‹œê°„ ì¶”ì²œ ì˜ˆì¸¡
"""

import asyncio
import logging
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import json
import pickle
import os
from dataclasses import dataclass

logger = logging.getLogger("CarFin-MCP.NCFPredict")

class NCFPredictError(Exception):
    """NCF ì˜ˆì¸¡ ê´€ë ¨ ì—ëŸ¬"""
    pass

@dataclass
class UserEmbedding:
    user_id: str
    age_group: int
    income_level: int
    location: str
    preferences: Dict[str, float]
    behavior_vector: List[float]

@dataclass
class ItemEmbedding:
    item_id: str
    price: float
    brand_encoded: int
    model_year: int
    fuel_type_encoded: int
    features_vector: List[float]

class NCFModel(nn.Module):
    """Neural Collaborative Filtering ëª¨ë¸ (He et al., 2017)"""

    def __init__(self, num_users: int, num_items: int, embedding_dim: int = 64, hidden_dims: List[int] = [128, 64, 32]):
        super(NCFModel, self).__init__()

        # GMF (Generalized Matrix Factorization) ë¶€ë¶„
        self.user_embedding_gmf = nn.Embedding(num_users, embedding_dim)
        self.item_embedding_gmf = nn.Embedding(num_items, embedding_dim)

        # MLP (Multi-Layer Perceptron) ë¶€ë¶„
        self.user_embedding_mlp = nn.Embedding(num_users, embedding_dim)
        self.item_embedding_mlp = nn.Embedding(num_items, embedding_dim)

        # MLP ë ˆì´ì–´ êµ¬ì„±
        mlp_input_dim = embedding_dim * 2
        mlp_layers = []
        for i, hidden_dim in enumerate(hidden_dims):
            if i == 0:
                mlp_layers.extend([
                    nn.Linear(mlp_input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.2)
                ])
            else:
                mlp_layers.extend([
                    nn.Linear(hidden_dims[i-1], hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.2)
                ])

        self.mlp_layers = nn.Sequential(*mlp_layers)

        # ìµœì¢… ì˜ˆì¸¡ ë ˆì´ì–´
        final_input_dim = embedding_dim + hidden_dims[-1]
        self.prediction = nn.Sequential(
            nn.Linear(final_input_dim, 1),
            nn.Sigmoid()
        )

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()

    def _init_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Embedding):
                nn.init.normal_(module.weight, mean=0.0, std=0.01)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0.0)

    def forward(self, user_ids: torch.Tensor, item_ids: torch.Tensor) -> torch.Tensor:
        # GMF ë¶€ë¶„
        user_emb_gmf = self.user_embedding_gmf(user_ids)
        item_emb_gmf = self.item_embedding_gmf(item_ids)
        gmf_output = user_emb_gmf * item_emb_gmf

        # MLP ë¶€ë¶„
        user_emb_mlp = self.user_embedding_mlp(user_ids)
        item_emb_mlp = self.item_embedding_mlp(item_ids)
        mlp_input = torch.cat([user_emb_mlp, item_emb_mlp], dim=-1)
        mlp_output = self.mlp_layers(mlp_input)

        # ê²°í•© ë° ì˜ˆì¸¡
        final_input = torch.cat([gmf_output, mlp_output], dim=-1)
        prediction = self.prediction(final_input)

        return prediction.squeeze()

class NCFPredictTool:
    """NCF ê¸°ë°˜ ì‹¤ì‹œê°„ ì¶”ì²œ ì˜ˆì¸¡ ë„êµ¬"""

    def __init__(self):
        # ëª¨ë¸ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = None
        self.user_encoder = {}  # user_id -> index
        self.item_encoder = {}  # item_id -> index
        self.model_metadata = {}

        # ëª¨ë¸ ê²½ë¡œ
        self.model_dir = "models"
        self.model_path = os.path.join(self.model_dir, "ncf_model.pth")
        self.encoders_path = os.path.join(self.model_dir, "encoders.pkl")

        # ì‹¤ì‹œê°„ í•™ìŠµ ì„¤ì •
        self.learning_rate = 0.001
        self.batch_size = 256
        self.online_buffer = []  # ì‹¤ì‹œê°„ í”¼ë“œë°± ì €ì¥
        self.buffer_limit = 1000

        # ì„±ëŠ¥ í†µê³„
        self.prediction_stats = {
            "total_predictions": 0,
            "avg_prediction_time": 0.0,
            "cache_hits": 0,
            "model_updates": 0
        }

        logger.info("âœ… NCF Predict Tool ì´ˆê¸°í™” ì™„ë£Œ")

        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        asyncio.create_task(self._load_or_init_model())

    async def _load_or_init_model(self):
        """ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”"""
        try:
            if os.path.exists(self.model_path) and os.path.exists(self.encoders_path):
                await self._load_model()
                logger.info("ğŸ”„ ê¸°ì¡´ NCF ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                await self._initialize_new_model()
                logger.info("ğŸ†• ìƒˆë¡œìš´ NCF ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def _load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        # ì¸ì½”ë” ë¡œë“œ
        with open(self.encoders_path, 'rb') as f:
            encoders_data = pickle.load(f)
            self.user_encoder = encoders_data['user_encoder']
            self.item_encoder = encoders_data['item_encoder']
            self.model_metadata = encoders_data['metadata']

        # ëª¨ë¸ ë¡œë“œ
        num_users = len(self.user_encoder)
        num_items = len(self.item_encoder)

        self.model = NCFModel(
            num_users=num_users,
            num_items=num_items,
            embedding_dim=self.model_metadata.get('embedding_dim', 64),
            hidden_dims=self.model_metadata.get('hidden_dims', [128, 64, 32])
        ).to(self.device)

        self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))
        self.model.eval()

    async def _initialize_new_model(self):
        """ìƒˆë¡œìš´ ëª¨ë¸ ì´ˆê¸°í™”"""
        # ê¸°ë³¸ ì¸ì½”ë” ì„¤ì • (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        self.user_encoder = {"dummy_user": 0}
        self.item_encoder = {"dummy_item": 0}
        self.model_metadata = {
            'embedding_dim': 64,
            'hidden_dims': [128, 64, 32],
            'created_at': datetime.now().isoformat()
        }

        # ê¸°ë³¸ ëª¨ë¸ ìƒì„±
        self.model = NCFModel(
            num_users=1000,  # ì´ˆê¸° ìš©ëŸ‰
            num_items=10000, # ì´ˆê¸° ìš©ëŸ‰
            embedding_dim=64,
            hidden_dims=[128, 64, 32]
        ).to(self.device)

        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.model_dir, exist_ok=True)

    async def execute(self, params: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        NCF ì˜ˆì¸¡ ì‹¤í–‰

        params:
            action: 'predict' | 'batch_predict' | 'update_feedback' | 'get_stats'
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„
            item_candidates: ì¶”ì²œ í›„ë³´ ì•„ì´í…œë“¤
            feedback_data: í”¼ë“œë°± ë°ì´í„° (í•™ìŠµìš©)
        """
        try:
            action = params.get("action", "predict")

            if action == "predict":
                return await self._predict_single(params)
            elif action == "batch_predict":
                return await self._predict_batch(params)
            elif action == "update_feedback":
                return await self._update_feedback(params)
            elif action == "get_stats":
                return await self._get_prediction_stats()
            else:
                raise NCFPredictError(f"Unknown action: {action}")

        except Exception as e:
            logger.error(f"âŒ NCF ì˜ˆì¸¡ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise NCFPredictError(f"Prediction execution failed: {e}")

    async def _predict_single(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì‚¬ìš©ì-ì•„ì´í…œ ì˜ˆì¸¡"""
        start_time = datetime.now()

        user_profile = params.get("user_profile", {})
        item_candidates = params.get("item_candidates", [])

        if not user_profile or not item_candidates:
            raise NCFPredictError("Missing user_profile or item_candidates")

        # ì‚¬ìš©ì ì¸ì½”ë”©
        user_id = user_profile.get("user_id", "anonymous")
        user_idx = self._encode_user(user_id, user_profile)

        predictions = []

        for item in item_candidates:
            item_id = item.get("vehicleid") or item.get("item_id")
            if not item_id:
                continue

            # ì•„ì´í…œ ì¸ì½”ë”©
            item_idx = self._encode_item(item_id, item)

            # ì˜ˆì¸¡ ìˆ˜í–‰
            with torch.no_grad():
                user_tensor = torch.tensor([user_idx], device=self.device)
                item_tensor = torch.tensor([item_idx], device=self.device)

                score = self.model(user_tensor, item_tensor).item()

                # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€ë¡œ ì ìˆ˜ ì¡°ì •
                adjusted_score = self._adjust_score_with_context(
                    score, user_profile, item, context
                )

                predictions.append({
                    "item_id": item_id,
                    "score": adjusted_score,
                    "raw_ncf_score": score,
                    "item_features": self._extract_item_features(item)
                })

        # ì ìˆ˜ìˆœ ì •ë ¬
        predictions.sort(key=lambda x: x["score"], reverse=True)

        # í†µê³„ ì—…ë°ì´íŠ¸
        execution_time = (datetime.now() - start_time).total_seconds()
        self._update_stats(execution_time, len(predictions))

        logger.info(f"ğŸ¯ NCF ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ì•„ì´í…œ, {execution_time:.3f}ì´ˆ")

        return {
            "success": True,
            "predictions": predictions,
            "execution_time": execution_time,
            "model_info": {
                "device": str(self.device),
                "users_encoded": len(self.user_encoder),
                "items_encoded": len(self.item_encoder)
            }
        }

    async def _predict_batch(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ë°°ì¹˜ ì˜ˆì¸¡ (ì—¬ëŸ¬ ì‚¬ìš©ì)"""
        start_time = datetime.now()

        user_profiles = params.get("user_profiles", [])
        item_candidates = params.get("item_candidates", [])

        batch_predictions = {}

        for user_profile in user_profiles:
            user_result = await self._predict_single({
                "user_profile": user_profile,
                "item_candidates": item_candidates
            })

            user_id = user_profile.get("user_id", "anonymous")
            batch_predictions[user_id] = user_result["predictions"]

        execution_time = (datetime.now() - start_time).total_seconds()

        return {
            "success": True,
            "batch_predictions": batch_predictions,
            "total_users": len(user_profiles),
            "execution_time": execution_time
        }

    async def _update_feedback(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        feedback_data = params.get("feedback_data", [])

        if not feedback_data:
            return {"success": True, "message": "No feedback data provided"}

        # ì˜¨ë¼ì¸ ë²„í¼ì— ì¶”ê°€
        self.online_buffer.extend(feedback_data)

        # ë²„í¼ í¬ê¸° ê´€ë¦¬
        if len(self.online_buffer) > self.buffer_limit:
            self.online_buffer = self.online_buffer[-self.buffer_limit:]

        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìŒ“ì´ë©´ ëª¨ë¸ ì—…ë°ì´íŠ¸
        if len(self.online_buffer) >= 100:  # ë°°ì¹˜ í¬ê¸°
            await self._perform_online_learning()

        return {
            "success": True,
            "feedback_added": len(feedback_data),
            "buffer_size": len(self.online_buffer),
            "model_updated": len(self.online_buffer) >= 100
        }

    async def _perform_online_learning(self):
        """ì˜¨ë¼ì¸ í•™ìŠµ ìˆ˜í–‰"""
        try:
            logger.info("ğŸ”„ ì˜¨ë¼ì¸ í•™ìŠµ ì‹œì‘")

            # í”¼ë“œë°± ë°ì´í„° ì¤€ë¹„
            training_data = []
            for feedback in self.online_buffer[-100:]:  # ìµœê·¼ 100ê°œ
                user_id = feedback.get("user_id")
                item_id = feedback.get("item_id")
                rating = feedback.get("rating", 0.5)  # í´ë¦­: 1, ë¬´ì‹œ: 0

                if user_id and item_id:
                    user_idx = self.user_encoder.get(user_id, 0)
                    item_idx = self.item_encoder.get(item_id, 0)
                    training_data.append((user_idx, item_idx, rating))

            if len(training_data) < 10:
                return

            # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
            self.model.train()
            optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
            criterion = nn.MSELoss()

            # ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ
            for epoch in range(5):  # ë¹ ë¥¸ ì˜¨ë¼ì¸ í•™ìŠµ
                total_loss = 0

                for i in range(0, len(training_data), 32):
                    batch = training_data[i:i+32]

                    user_ids = torch.tensor([item[0] for item in batch], device=self.device)
                    item_ids = torch.tensor([item[1] for item in batch], device=self.device)
                    ratings = torch.tensor([item[2] for item in batch], dtype=torch.float32, device=self.device)

                    optimizer.zero_grad()
                    predictions = self.model(user_ids, item_ids)
                    loss = criterion(predictions, ratings)
                    loss.backward()
                    optimizer.step()

                    total_loss += loss.item()

                logger.info(f"ğŸ“ˆ Epoch {epoch+1}, Loss: {total_loss:.4f}")

            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
            self.model.eval()

            # í†µê³„ ì—…ë°ì´íŠ¸
            self.prediction_stats["model_updates"] += 1

            logger.info("âœ… ì˜¨ë¼ì¸ í•™ìŠµ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ì˜¨ë¼ì¸ í•™ìŠµ ì‹¤íŒ¨: {e}")

    def _encode_user(self, user_id: str, user_profile: Dict[str, Any]) -> int:
        """ì‚¬ìš©ì ì¸ì½”ë”©"""
        if user_id not in self.user_encoder:
            # ìƒˆë¡œìš´ ì‚¬ìš©ì ì¶”ê°€
            new_idx = len(self.user_encoder)
            self.user_encoder[user_id] = new_idx

        return self.user_encoder[user_id]

    def _encode_item(self, item_id: str, item_info: Dict[str, Any]) -> int:
        """ì•„ì´í…œ ì¸ì½”ë”©"""
        if item_id not in self.item_encoder:
            # ìƒˆë¡œìš´ ì•„ì´í…œ ì¶”ê°€
            new_idx = len(self.item_encoder)
            self.item_encoder[item_id] = new_idx

        return self.item_encoder[item_id]

    def _adjust_score_with_context(
        self,
        base_score: float,
        user_profile: Dict[str, Any],
        item: Dict[str, Any],
        context: Optional[Dict[str, Any]]
    ) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¡œ ì ìˆ˜ ì¡°ì •"""
        adjusted_score = base_score

        # ì˜ˆì‚° ì í•©ì„± ì¡°ì •
        user_budget = user_profile.get("budget", {})
        item_price = item.get("price", 0)

        if user_budget:
            min_budget = user_budget.get("min", 0)
            max_budget = user_budget.get("max", float('inf'))

            if min_budget <= item_price <= max_budget:
                adjusted_score *= 1.1  # ì˜ˆì‚° ë‚´ ì°¨ëŸ‰ ê°€ì‚°ì 
            elif item_price > max_budget:
                adjusted_score *= 0.7  # ì˜ˆì‚° ì´ˆê³¼ ê°ì 

        # ì„ í˜¸ ë¸Œëœë“œ ì¡°ì •
        preferred_brands = user_profile.get("preferred_brands", [])
        item_brand = item.get("manufacturer", "")

        if item_brand in preferred_brands:
            adjusted_score *= 1.15

        # ì—°ì‹ ì„ í˜¸ë„ ì¡°ì •
        current_year = 2025
        item_year = item.get("modelyear", current_year)
        age = current_year - item_year

        if age <= 3:  # 3ë…„ ì´í•˜ ì°¨ëŸ‰
            adjusted_score *= 1.05
        elif age >= 10:  # 10ë…„ ì´ìƒ ì°¨ëŸ‰
            adjusted_score *= 0.9

        return min(adjusted_score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ

    def _extract_item_features(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """ì•„ì´í…œ íŠ¹ì„± ì¶”ì¶œ"""
        return {
            "price": item.get("price", 0),
            "year": item.get("modelyear", 2020),
            "mileage": item.get("distance", 0),
            "fuel_type": item.get("fueltype", ""),
            "brand": item.get("manufacturer", ""),
            "car_type": item.get("cartype", "")
        }

    def _update_stats(self, execution_time: float, num_predictions: int):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.prediction_stats["total_predictions"] += num_predictions

        # í‰ê·  ì˜ˆì¸¡ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_ops = self.prediction_stats["total_predictions"]
        current_avg = self.prediction_stats["avg_prediction_time"]
        self.prediction_stats["avg_prediction_time"] = (
            (current_avg * (total_ops - num_predictions) + execution_time) / total_ops
        )

    async def _get_prediction_stats(self) -> Dict[str, Any]:
        """ì˜ˆì¸¡ í†µê³„ ë°˜í™˜"""
        return {
            "success": True,
            "stats": self.prediction_stats,
            "model_info": {
                "users_encoded": len(self.user_encoder),
                "items_encoded": len(self.item_encoder),
                "device": str(self.device),
                "buffer_size": len(self.online_buffer)
            }
        }

    async def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        try:
            # ëª¨ë¸ ì €ì¥
            torch.save(self.model.state_dict(), self.model_path)

            # ì¸ì½”ë” ì €ì¥
            encoders_data = {
                'user_encoder': self.user_encoder,
                'item_encoder': self.item_encoder,
                'metadata': self.model_metadata
            }

            with open(self.encoders_path, 'wb') as f:
                pickle.dump(encoders_data, f)

            logger.info("ğŸ’¾ NCF ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ncf_predict_tool = NCFPredictTool()

# MCP Tool ì¸í„°í˜ì´ìŠ¤
async def carfin_ncf_predict(params: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    CarFin MCP Tool: NCF ì˜ˆì¸¡

    ì‚¬ìš© ì˜ˆì‹œ:
    - ë‹¨ì¼ ì˜ˆì¸¡: {"action": "predict", "user_profile": {...}, "item_candidates": [...]}
    - ë°°ì¹˜ ì˜ˆì¸¡: {"action": "batch_predict", "user_profiles": [...], "item_candidates": [...]}
    - í”¼ë“œë°± í•™ìŠµ: {"action": "update_feedback", "feedback_data": [...]}
    """
    return await ncf_predict_tool.execute(params, context)