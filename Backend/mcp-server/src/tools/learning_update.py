"""
CarFin MCP Tool: Learning Update
ì‹¤ì‹œê°„ ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ì—…ë°ì´íŠ¸
"""

import asyncio
import logging
import torch
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta
import json
import sqlite3
from dataclasses import dataclass, asdict
from collections import defaultdict

logger = logging.getLogger("CarFin-MCP.LearningUpdate")

class LearningUpdateError(Exception):
    """í•™ìŠµ ì—…ë°ì´íŠ¸ ê´€ë ¨ ì—ëŸ¬"""
    pass

@dataclass
class UserFeedback:
    user_id: str
    item_id: str
    feedback_type: str  # 'click', 'view', 'like', 'dislike', 'purchase_inquiry'
    rating: float  # 0.0 ~ 1.0
    session_id: str
    timestamp: datetime
    context: Dict[str, Any]

@dataclass
class LearningBatch:
    batch_id: str
    feedbacks: List[UserFeedback]
    created_at: datetime
    model_version: str
    processed: bool = False

class LearningUpdateTool:
    """ì‹¤ì‹œê°„ í•™ìŠµ ì—…ë°ì´íŠ¸ ë„êµ¬"""

    def __init__(self):
        # í”¼ë“œë°± ì €ì¥ì†Œ
        self.feedback_db_path = "feedback.db"
        self.init_feedback_database()

        # í•™ìŠµ ì„¤ì •
        self.learning_config = {
            "batch_size": 64,
            "learning_rate": 0.0001,
            "update_frequency": 100,  # 100ê°œ í”¼ë“œë°±ë§ˆë‹¤ ì—…ë°ì´íŠ¸
            "forgetting_factor": 0.95,  # ê³¼ê±° í•™ìŠµ ê°€ì¤‘ì¹˜
            "min_feedback_threshold": 10
        }

        # ì‹¤ì‹œê°„ í”¼ë“œë°± ë²„í¼
        self.feedback_buffer = []
        self.user_interaction_history = defaultdict(list)

        # í•™ìŠµ í†µê³„
        self.learning_stats = {
            "total_feedbacks": 0,
            "model_updates": 0,
            "last_update": None,
            "avg_update_time": 0.0,
            "feedback_types_count": defaultdict(int),
            "user_engagement_metrics": defaultdict(dict)
        }

        # A/B í…ŒìŠ¤íŠ¸ ì„¤ì •
        self.ab_test_groups = {
            "control": 0.5,    # ê¸°ì¡´ ëª¨ë¸
            "experimental": 0.5  # ì—…ë°ì´íŠ¸ëœ ëª¨ë¸
        }

        logger.info("âœ… LearningUpdate Tool ì´ˆê¸°í™” ì™„ë£Œ")

    def init_feedback_database(self):
        """í”¼ë“œë°± ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            conn = sqlite3.connect(self.feedback_db_path)
            cursor = conn.cursor()

            # í”¼ë“œë°± í…Œì´ë¸” ìƒì„±
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS user_feedback (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT NOT NULL,
                    item_id TEXT NOT NULL,
                    feedback_type TEXT NOT NULL,
                    rating REAL NOT NULL,
                    session_id TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    context TEXT,
                    processed BOOLEAN DEFAULT FALSE
                )
            ''')

            # í•™ìŠµ ë°°ì¹˜ í…Œì´ë¸” ìƒì„±
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS learning_batches (
                    batch_id TEXT PRIMARY KEY,
                    created_at TEXT NOT NULL,
                    model_version TEXT NOT NULL,
                    feedback_count INTEGER NOT NULL,
                    processed BOOLEAN DEFAULT FALSE,
                    performance_metrics TEXT
                )
            ''')

            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_user_feedback_timestamp ON user_feedback(timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_user_feedback_user_id ON user_feedback(user_id)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_user_feedback_processed ON user_feedback(processed)')

            conn.commit()
            conn.close()

            logger.info("ğŸ“Š í”¼ë“œë°± ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def execute(self, params: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        í•™ìŠµ ì—…ë°ì´íŠ¸ ì‹¤í–‰

        params:
            action: 'record_feedback' | 'trigger_update' | 'get_stats' | 'get_user_profile'
            feedback_data: ì‚¬ìš©ì í”¼ë“œë°± ë°ì´í„°
            force_update: ê°•ì œ ì—…ë°ì´íŠ¸ ì—¬ë¶€
            user_id: ì‚¬ìš©ì ID (í”„ë¡œí•„ ì¡°íšŒìš©)
        """
        try:
            action = params.get("action", "record_feedback")

            if action == "record_feedback":
                return await self._record_user_feedback(params)
            elif action == "trigger_update":
                return await self._trigger_model_update(params)
            elif action == "get_stats":
                return await self._get_learning_stats()
            elif action == "get_user_profile":
                return await self._get_user_learning_profile(params)
            elif action == "ab_test_assignment":
                return await self._assign_ab_test_group(params)
            else:
                raise LearningUpdateError(f"Unknown action: {action}")

        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì—…ë°ì´íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise LearningUpdateError(f"Learning update execution failed: {e}")

    async def _record_user_feedback(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë¡"""
        feedback_data = params.get("feedback_data", {})

        if not feedback_data:
            raise LearningUpdateError("No feedback data provided")

        # í”¼ë“œë°± ê°ì²´ ìƒì„±
        user_feedback = UserFeedback(
            user_id=feedback_data.get("user_id", "anonymous"),
            item_id=feedback_data.get("item_id", ""),
            feedback_type=feedback_data.get("feedback_type", "view"),
            rating=self._convert_feedback_to_rating(
                feedback_data.get("feedback_type", "view"),
                feedback_data.get("explicit_rating", None)
            ),
            session_id=feedback_data.get("session_id", ""),
            timestamp=datetime.now(),
            context=feedback_data.get("context", {})
        )

        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        await self._save_feedback_to_db(user_feedback)

        # ë²„í¼ì— ì¶”ê°€
        self.feedback_buffer.append(user_feedback)

        # ì‚¬ìš©ì ìƒí˜¸ì‘ìš© íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.user_interaction_history[user_feedback.user_id].append(user_feedback)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.learning_stats["total_feedbacks"] += 1
        self.learning_stats["feedback_types_count"][user_feedback.feedback_type] += 1

        # ìë™ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±° í™•ì¸
        should_update = len(self.feedback_buffer) >= self.learning_config["update_frequency"]

        if should_update:
            asyncio.create_task(self._trigger_model_update({"force_update": False}))

        logger.info(f"ğŸ“ í”¼ë“œë°± ê¸°ë¡: {user_feedback.user_id} â†’ {user_feedback.item_id} ({user_feedback.feedback_type})")

        return {
            "success": True,
            "feedback_recorded": True,
            "buffer_size": len(self.feedback_buffer),
            "should_update": should_update,
            "feedback_id": f"{user_feedback.user_id}_{user_feedback.timestamp.isoformat()}"
        }

    async def _save_feedback_to_db(self, feedback: UserFeedback):
        """í”¼ë“œë°±ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            conn = sqlite3.connect(self.feedback_db_path)
            cursor = conn.cursor()

            cursor.execute('''
                INSERT INTO user_feedback
                (user_id, item_id, feedback_type, rating, session_id, timestamp, context)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                feedback.user_id,
                feedback.item_id,
                feedback.feedback_type,
                feedback.rating,
                feedback.session_id,
                feedback.timestamp.isoformat(),
                json.dumps(feedback.context)
            ))

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"âŒ í”¼ë“œë°± ì €ì¥ ì‹¤íŒ¨: {e}")

    def _convert_feedback_to_rating(self, feedback_type: str, explicit_rating: Optional[float]) -> float:
        """í”¼ë“œë°± íƒ€ì…ì„ í‰ì ìœ¼ë¡œ ë³€í™˜"""
        if explicit_rating is not None:
            return max(0.0, min(1.0, explicit_rating))

        # ì•”ì‹œì  í”¼ë“œë°± ë³€í™˜
        feedback_ratings = {
            "view": 0.1,           # ë‹¨ìˆœ ì¡°íšŒ
            "click": 0.3,          # í´ë¦­
            "detail_view": 0.5,    # ìƒì„¸ ì¡°íšŒ
            "like": 0.8,           # ì¢‹ì•„ìš”
            "favorite": 0.9,       # ì°œí•˜ê¸°
            "purchase_inquiry": 1.0,  # êµ¬ë§¤ ë¬¸ì˜
            "dislike": 0.0,        # ì‹«ì–´ìš”
            "skip": 0.05           # ê±´ë„ˆë›°ê¸°
        }

        return feedback_ratings.get(feedback_type, 0.1)

    async def _trigger_model_update(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±°"""
        start_time = datetime.now()

        force_update = params.get("force_update", False)

        # ì—…ë°ì´íŠ¸ ì¡°ê±´ í™•ì¸
        if not force_update and len(self.feedback_buffer) < self.learning_config["min_feedback_threshold"]:
            return {
                "success": True,
                "updated": False,
                "reason": "Insufficient feedback data",
                "buffer_size": len(self.feedback_buffer)
            }

        try:
            logger.info("ğŸ”„ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œì‘")

            # 1. í•™ìŠµ ë°°ì¹˜ ìƒì„±
            batch = await self._create_learning_batch()

            # 2. NCF ëª¨ë¸ ì—…ë°ì´íŠ¸
            ncf_update_result = await self._update_ncf_model(batch)

            # 3. ì—ì´ì „íŠ¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            agent_weights_result = await self._update_agent_weights(batch)

            # 4. ê°œì¸í™” í”„ë¡œí•„ ì—…ë°ì´íŠ¸
            personalization_result = await self._update_user_profiles(batch)

            # 5. A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
            ab_test_result = await self._analyze_ab_test_performance()

            # í†µê³„ ì—…ë°ì´íŠ¸
            execution_time = (datetime.now() - start_time).total_seconds()
            self.learning_stats["model_updates"] += 1
            self.learning_stats["last_update"] = datetime.now().isoformat()

            # í‰ê·  ì—…ë°ì´íŠ¸ ì‹œê°„ ê³„ì‚°
            update_count = self.learning_stats["model_updates"]
            current_avg = self.learning_stats["avg_update_time"]
            self.learning_stats["avg_update_time"] = (
                (current_avg * (update_count - 1) + execution_time) / update_count
            )

            # ë²„í¼ ì´ˆê¸°í™”
            self.feedback_buffer = []

            logger.info(f"âœ… ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {execution_time:.3f}ì´ˆ")

            return {
                "success": True,
                "updated": True,
                "execution_time": execution_time,
                "batch_id": batch.batch_id,
                "update_results": {
                    "ncf_model": ncf_update_result,
                    "agent_weights": agent_weights_result,
                    "personalization": personalization_result,
                    "ab_test": ab_test_result
                },
                "feedbacks_processed": len(batch.feedbacks)
            }

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "execution_time": (datetime.now() - start_time).total_seconds()
            }

    async def _create_learning_batch(self) -> LearningBatch:
        """í•™ìŠµ ë°°ì¹˜ ìƒì„±"""
        batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        batch = LearningBatch(
            batch_id=batch_id,
            feedbacks=self.feedback_buffer.copy(),
            created_at=datetime.now(),
            model_version="v1.0-beta"
        )

        # ë°°ì¹˜ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        try:
            conn = sqlite3.connect(self.feedback_db_path)
            cursor = conn.cursor()

            cursor.execute('''
                INSERT INTO learning_batches
                (batch_id, created_at, model_version, feedback_count)
                VALUES (?, ?, ?, ?)
            ''', (
                batch.batch_id,
                batch.created_at.isoformat(),
                batch.model_version,
                len(batch.feedbacks)
            ))

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")

        return batch

    async def _update_ncf_model(self, batch: LearningBatch) -> Dict[str, Any]:
        """NCF ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        try:
            # í”¼ë“œë°± ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ ë³€í™˜
            training_data = []

            for feedback in batch.feedbacks:
                training_data.append({
                    "user_id": feedback.user_id,
                    "item_id": feedback.item_id,
                    "rating": feedback.rating,
                    "timestamp": feedback.timestamp
                })

            # NCF ì˜ˆì¸¡ ë„êµ¬ë¥¼ í†µí•´ ì˜¨ë¼ì¸ í•™ìŠµ ìˆ˜í–‰
            # (ì‹¤ì œë¡œëŠ” ncf_predict_toolì„ importí•´ì„œ ì‚¬ìš©)
            update_params = {
                "action": "update_feedback",
                "feedback_data": training_data
            }

            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì²˜ë¦¬
            await asyncio.sleep(0.1)  # ì‹¤ì œ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜

            return {
                "updated": True,
                "training_samples": len(training_data),
                "learning_rate": self.learning_config["learning_rate"],
                "model_improvement": 0.02  # ëª¨ì˜ ê°œì„ ìœ¨
            }

        except Exception as e:
            logger.error(f"âŒ NCF ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return {"updated": False, "error": str(e)}

    async def _update_agent_weights(self, batch: LearningBatch) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"""
        try:
            # í”¼ë“œë°± ê¸°ë°˜ ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¶„ì„
            agent_performance = defaultdict(list)

            for feedback in batch.feedbacks:
                # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì²œ ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
                context = feedback.context
                recommending_agent = context.get("recommending_agent", "unknown")

                if recommending_agent != "unknown":
                    agent_performance[recommending_agent].append(feedback.rating)

            # ì—ì´ì „íŠ¸ë³„ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
            agent_scores = {}
            for agent, ratings in agent_performance.items():
                if ratings:
                    agent_scores[agent] = {
                        "avg_rating": np.mean(ratings),
                        "sample_count": len(ratings),
                        "performance_trend": "improving" if np.mean(ratings) > 0.5 else "declining"
                    }

            # ì¶”ì²œ ìœµí•© ë„êµ¬ì— ì„±ëŠ¥ ì—…ë°ì´íŠ¸ ì „ë‹¬
            # (ì‹¤ì œë¡œëŠ” recommendation_fuse_toolì„ importí•´ì„œ ì‚¬ìš©)

            return {
                "updated": True,
                "agent_scores": agent_scores,
                "total_feedback_analyzed": len(batch.feedbacks)
            }

        except Exception as e:
            logger.error(f"âŒ ì—ì´ì „íŠ¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return {"updated": False, "error": str(e)}

    async def _update_user_profiles(self, batch: LearningBatch) -> Dict[str, Any]:
        """ì‚¬ìš©ì ê°œì¸í™” í”„ë¡œí•„ ì—…ë°ì´íŠ¸"""
        try:
            user_profiles_updated = set()

            # ì‚¬ìš©ìë³„ í”¼ë“œë°± ë¶„ì„
            user_feedback_analysis = defaultdict(list)

            for feedback in batch.feedbacks:
                user_feedback_analysis[feedback.user_id].append(feedback)

            # ê° ì‚¬ìš©ìì˜ ì„ í˜¸ë„ í”„ë¡œí•„ ì—…ë°ì´íŠ¸
            for user_id, feedbacks in user_feedback_analysis.items():
                # ì„ í˜¸ ë¸Œëœë“œ ë¶„ì„
                brand_preferences = defaultdict(list)
                price_preferences = []
                year_preferences = []

                for feedback in feedbacks:
                    context = feedback.context
                    item_info = context.get("item_info", {})

                    brand = item_info.get("manufacturer", "")
                    price = item_info.get("price", 0)
                    year = item_info.get("modelyear", 0)

                    if brand:
                        brand_preferences[brand].append(feedback.rating)
                    if price > 0:
                        price_preferences.append((price, feedback.rating))
                    if year > 0:
                        year_preferences.append((year, feedback.rating))

                # ì„ í˜¸ë„ í”„ë¡œí•„ ê³„ì‚°
                user_profile = {
                    "preferred_brands": self._analyze_brand_preferences(brand_preferences),
                    "price_sensitivity": self._analyze_price_preferences(price_preferences),
                    "year_preferences": self._analyze_year_preferences(year_preferences),
                    "updated_at": datetime.now().isoformat()
                }

                # í”„ë¡œí•„ ì €ì¥ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ìºì‹œì— ì €ì¥)
                user_profiles_updated.add(user_id)

            return {
                "updated": True,
                "profiles_updated": len(user_profiles_updated),
                "user_ids": list(user_profiles_updated)
            }

        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return {"updated": False, "error": str(e)}

    def _analyze_brand_preferences(self, brand_preferences: Dict[str, List[float]]) -> List[str]:
        """ë¸Œëœë“œ ì„ í˜¸ë„ ë¶„ì„"""
        preferred_brands = []

        for brand, ratings in brand_preferences.items():
            avg_rating = np.mean(ratings)
            if avg_rating > 0.6 and len(ratings) >= 2:  # ì¶©ë¶„í•œ ë°ì´í„°ì™€ ë†’ì€ í‰ì 
                preferred_brands.append(brand)

        return preferred_brands

    def _analyze_price_preferences(self, price_preferences: List[Tuple[float, float]]) -> Dict[str, Any]:
        """ê°€ê²© ì„ í˜¸ë„ ë¶„ì„"""
        if not price_preferences:
            return {"sensitivity": "medium", "preferred_range": None}

        # ë†’ì€ í‰ì ì„ ë°›ì€ ê°€ê²©ëŒ€ ë¶„ì„
        high_rated_prices = [price for price, rating in price_preferences if rating > 0.6]

        if high_rated_prices:
            return {
                "sensitivity": "low" if np.std(high_rated_prices) > 1000 else "high",
                "preferred_range": {
                    "min": int(np.percentile(high_rated_prices, 25)),
                    "max": int(np.percentile(high_rated_prices, 75))
                }
            }

        return {"sensitivity": "medium", "preferred_range": None}

    def _analyze_year_preferences(self, year_preferences: List[Tuple[int, float]]) -> Dict[str, Any]:
        """ì—°ì‹ ì„ í˜¸ë„ ë¶„ì„"""
        if not year_preferences:
            return {"preference_type": "neutral", "preferred_range": None}

        # ë†’ì€ í‰ì ì„ ë°›ì€ ì—°ì‹ë“¤ ë¶„ì„
        high_rated_years = [year for year, rating in year_preferences if rating > 0.6]

        if high_rated_years:
            avg_year = np.mean(high_rated_years)
            current_year = 2025

            if avg_year >= current_year - 3:
                preference_type = "new_cars"
            elif avg_year <= current_year - 10:
                preference_type = "vintage_cars"
            else:
                preference_type = "balanced"

            return {
                "preference_type": preference_type,
                "preferred_range": {
                    "min": int(np.min(high_rated_years)),
                    "max": int(np.max(high_rated_years))
                }
            }

        return {"preference_type": "neutral", "preferred_range": None}

    async def _analyze_ab_test_performance(self) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ì„±ê³¼ ë¶„ì„"""
        try:
            # ìµœê·¼ 7ì¼ê°„ í”¼ë“œë°± ë°ì´í„° ì¡°íšŒ
            seven_days_ago = datetime.now() - timedelta(days=7)

            conn = sqlite3.connect(self.feedback_db_path)
            cursor = conn.cursor()

            cursor.execute('''
                SELECT user_id, feedback_type, rating, context
                FROM user_feedback
                WHERE timestamp > ?
            ''', (seven_days_ago.isoformat(),))

            recent_feedbacks = cursor.fetchall()
            conn.close()

            # A/B ê·¸ë£¹ë³„ ì„±ê³¼ ë¶„ì„
            group_performance = defaultdict(list)

            for feedback in recent_feedbacks:
                context = json.loads(feedback[3]) if feedback[3] else {}
                ab_group = context.get("ab_test_group", "control")
                rating = feedback[2]

                group_performance[ab_group].append(rating)

            # í†µê³„ ê³„ì‚°
            performance_stats = {}
            for group, ratings in group_performance.items():
                if ratings:
                    performance_stats[group] = {
                        "avg_rating": np.mean(ratings),
                        "sample_size": len(ratings),
                        "std_dev": np.std(ratings),
                        "conversion_rate": len([r for r in ratings if r > 0.7]) / len(ratings)
                    }

            # í†µê³„ì  ìœ ì˜ì„± ê²€ì • (ê°„ë‹¨í•œ t-test)
            if "control" in performance_stats and "experimental" in performance_stats:
                control_avg = performance_stats["control"]["avg_rating"]
                experimental_avg = performance_stats["experimental"]["avg_rating"]

                improvement = ((experimental_avg - control_avg) / control_avg) * 100
                is_significant = abs(improvement) > 5.0  # 5% ì´ìƒ ì°¨ì´

                return {
                    "analyzed": True,
                    "performance_stats": performance_stats,
                    "improvement": improvement,
                    "is_significant": is_significant,
                    "recommendation": "deploy" if improvement > 5 else "continue_testing"
                }

            return {"analyzed": True, "performance_stats": performance_stats}

        except Exception as e:
            logger.error(f"âŒ A/B í…ŒìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"analyzed": False, "error": str(e)}

    async def _assign_ab_test_group(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ê·¸ë£¹ í• ë‹¹"""
        user_id = params.get("user_id", "anonymous")

        # ì‚¬ìš©ì ID ê¸°ë°˜ ì¼ê´€ì„± ìˆëŠ” ê·¸ë£¹ í• ë‹¹
        import hashlib
        user_hash = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        group_assignment = "experimental" if user_hash % 2 == 0 else "control"

        return {
            "success": True,
            "user_id": user_id,
            "assigned_group": group_assignment,
            "group_distribution": self.ab_test_groups
        }

    async def _get_learning_stats(self) -> Dict[str, Any]:
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        # ì‚¬ìš©ì ì°¸ì—¬ë„ ë©”íŠ¸ë¦­ ê³„ì‚°
        engagement_metrics = self._calculate_engagement_metrics()

        return {
            "success": True,
            "stats": self.learning_stats,
            "engagement_metrics": engagement_metrics,
            "buffer_status": {
                "current_size": len(self.feedback_buffer),
                "threshold": self.learning_config["update_frequency"],
                "fill_percentage": (len(self.feedback_buffer) / self.learning_config["update_frequency"]) * 100
            },
            "learning_config": self.learning_config
        }

    async def _get_user_learning_profile(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ì‚¬ìš©ì í•™ìŠµ í”„ë¡œí•„ ì¡°íšŒ"""
        user_id = params.get("user_id")

        if not user_id:
            raise LearningUpdateError("Missing user_id parameter")

        # ì‚¬ìš©ì ìƒí˜¸ì‘ìš© íˆìŠ¤í† ë¦¬ ì¡°íšŒ
        user_history = self.user_interaction_history.get(user_id, [])

        # í•™ìŠµëœ ì„ í˜¸ë„ í”„ë¡œí•„ ìƒì„±
        profile = self._generate_user_profile(user_history)

        return {
            "success": True,
            "user_id": user_id,
            "interaction_count": len(user_history),
            "learned_profile": profile,
            "last_activity": user_history[-1].timestamp.isoformat() if user_history else None
        }

    def _calculate_engagement_metrics(self) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì°¸ì—¬ë„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not self.user_interaction_history:
            return {"total_users": 0}

        total_users = len(self.user_interaction_history)
        total_interactions = sum(len(history) for history in self.user_interaction_history.values())

        # í™œì„± ì‚¬ìš©ì (ìµœê·¼ 7ì¼ ë‚´ í™œë™)
        recent_threshold = datetime.now() - timedelta(days=7)
        active_users = 0

        for user_history in self.user_interaction_history.values():
            if user_history and user_history[-1].timestamp > recent_threshold:
                active_users += 1

        return {
            "total_users": total_users,
            "active_users": active_users,
            "total_interactions": total_interactions,
            "avg_interactions_per_user": total_interactions / total_users if total_users > 0 else 0,
            "engagement_rate": (active_users / total_users) * 100 if total_users > 0 else 0
        }

    def _generate_user_profile(self, user_history: List[UserFeedback]) -> Dict[str, Any]:
        """ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ë¡œë¶€í„° í•™ìŠµ í”„ë¡œí•„ ìƒì„±"""
        if not user_history:
            return {"status": "no_data"}

        # í”¼ë“œë°± íƒ€ì…ë³„ í†µê³„
        feedback_types = defaultdict(int)
        total_rating = 0.0

        for feedback in user_history:
            feedback_types[feedback.feedback_type] += 1
            total_rating += feedback.rating

        avg_rating = total_rating / len(user_history)

        # ì‚¬ìš©ì í–‰ë™ íŒ¨í„´ ë¶„ì„
        behavior_pattern = "explorer" if len(feedback_types) > 3 else "focused"
        engagement_level = "high" if avg_rating > 0.6 else "medium" if avg_rating > 0.3 else "low"

        return {
            "status": "learned",
            "feedback_count": len(user_history),
            "avg_rating": avg_rating,
            "behavior_pattern": behavior_pattern,
            "engagement_level": engagement_level,
            "feedback_distribution": dict(feedback_types),
            "learning_confidence": min(len(user_history) / 20.0, 1.0)  # 20ê°œ í”¼ë“œë°±ìœ¼ë¡œ ì™„ì „ í•™ìŠµ
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
learning_update_tool = LearningUpdateTool()

# MCP Tool ì¸í„°í˜ì´ìŠ¤
async def carfin_learning_update(params: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    CarFin MCP Tool: í•™ìŠµ ì—…ë°ì´íŠ¸

    ì‚¬ìš© ì˜ˆì‹œ:
    - í”¼ë“œë°± ê¸°ë¡: {"action": "record_feedback", "feedback_data": {...}}
    - ëª¨ë¸ ì—…ë°ì´íŠ¸: {"action": "trigger_update", "force_update": true}
    - í†µê³„ ì¡°íšŒ: {"action": "get_stats"}
    - ì‚¬ìš©ì í”„ë¡œí•„: {"action": "get_user_profile", "user_id": "user123"}
    """
    return await learning_update_tool.execute(params, context)